\documentclass[11pt,a4paper]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{lipsum}
\usepackage{natbib}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{graphicx}

\title{Report: Dialog Act Classification\\\textit{using Word Embeddings \& Acoustic Features}}

\author{Jens Beck \\
  {\tt jens.beckl@ims} \\\And
  Fabian Fey \\
  {\tt fabian.fey@ims} \\\And
  Richard Kollotzek \\
  {\tt richard.kollotzek@ims} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
\lipsum[2-2]
\end{abstract}

\section{Introduction}
The general task is to classify lexical and auditory speech into one of four predefined \textit{dialog act classes}. A \textit{dialog act}, in this context, represents informal information of how a dialog system should respond to a users input. The four provided classes are \textit{statement, opinion, question} and \textit{backchannel}. To solve this task we developed \textit{convolutional neural networks} (CNN) that use lexical and acoustic features. For the development and training of the systems a subset of the \textit{Switchboard Dialog Act Corpus} was used. In next chapters we discuss the development of the systems and subsequently to that the research question \textbf{INSERT HERE}.

\section{Data \& Data Preperation}
In this section we discuss the \textit{Switchboard Dialog Act Corpus} and the extraction of the lexical and acoustic features.

	\subsection{The Switchboard Dialog Act Corpus}
	The \textit{Switchboard Dialog Act Corpus} \cite{switchboard}, from now on abbreviated as \textit{SwDA}, consists of recordings with corresponding transcripts. Each of these recordings is assigned to one of 42 \textit{dialog act classes}. For this project we reduced the amount of classes down to four which are \textit{statement, opinion, question} and \textit{backchannel}. These classes are supersets of the 42 \textit{dialog act classes} defined in the \textit{SwDA}. The distribution of the four classes within the training, development and test set are shown in Table \ref{tab:dataDistribution}.
	%todo Examples for the four classes if more text is needed
	\begin{table}[]
		\begin{tabular}{ r | c c c }
			& training & dev & test \\
			\hline
			opinion ($\sim$17\%) & 4984 & 1068 & 1070 \\
			question ($\sim$8\%) & 2150 & 460 & 463 \\
			backchannel ($\sim$24\%) & 6792 & 1455 & 1458  \\
			statement ($\sim$51\%) & 14459 & 3098 &  3099  \\
			\hline
			sum & 28385 & 6081 & 6090 \\
		\end{tabular}
		\caption{Displays the distribution of the four classes in the three data sets.}
		\label{tab:dataDistribution}
	\end{table}	
	The numbers illustrate a huge imbalance between the \textit{statement} class and the other three classes. However, we decided against reducing the data into equally distributed sets because this would exclude at least one third of the training data. This is important to keep in mind for the evaluation of the systems because an educated guess would have an accuracy of around 51\% by assigning all test examples to the \textit{statement} class.

	\subsection{Input Data Generation}
	Lexical and acoustic features were employed in our systems and had to be extracted and formatted into a machine readable format. For the lexical features we decided to use \textit{Google's} freely accessible word embeddings which were trained on 100 billion words \cite{word2vec}. As for the acoustic features we relied on \textit{Mel Frequency Cepstral Coefficient} (MFCC) features which were extracted with the \textit{openSMILE} feature extraction tool \cite{opensmile}. To ensure that data of different utterances could not be mixed the lexical and acoustic inputs were always stored with their respective one-hot vector in a triple data structure.
	
	\subsubsection*{Lexical Features}
	\label{sec:lexicalFeatures}
	The word embedding matrix $E$ was generated by assigning each word to its corresponding 300 dimensional vector of the \textit{Google word2vec} model. If a word was not included in the model it was assigned a randomly generated 300 dimensional vector. Furthermore, we introduced a padding vector for the case that a sentence was shorter than our maximum sentence length. We decided to restrict the length of a single utterance to 100 words to not exclude to much lexical features for long utterances. The final size of the embedding matrix $E$ was $11825  \times 300$.
	\begin{center}
		$ E= 
		\begin{bmatrix}
		v_{1,1}	&  \dots	 & v_{1,300}    \\
		v_{2,1}	 	& \dots  & v_{2,300} 	\\
		\vdots	 	& \ddots & \vdots		\\
		v_{i,1} 	& \dots	 & v_{i,300}
		\end{bmatrix}
		$
	\end{center}
	\vspace{0.3cm}
	The lexical input for our systems is a vector $x_{lex}$ were each word is represented by the index $i$ of its corresponding vector in the embedding matrix $E$. The vector $x_{lex}$ has a length of 100. Each element represents the index of a word in the utterance. The vector $x_{lex}$ has the following shape: $x_{lex} = [i_{1}, i_{2}, ..., i_{100}]$ where $i$ represents the index.
	
	\subsubsection*{Acoustic Features}
	Each utterance had its acoustic features formatted into a matrix $X_{aco}$ of shape $13 \times 2000$. This matrix was generated by arranging 2000 \textit{MFCC-frames} into a matrix. Each \textit{frame} hereby consisted of 13 coefficients. The chosen \textit{frames} were the first and last thousand \textit{frames} of the audio recording. If a recording had less than 2000 \textit{MFCC-frames} the matrix was padded with zero vectors.
	
	\begin{center}
		$ X_{aco}= 
		\begin{bmatrix}
		a_{1,1}	&  \dots	 & a_{1,2000}   \\
		a_{2,1}	 	& \dots  & a_{2,2000} 	\\
		\vdots	 	& \ddots & \vdots		\\
		a_{13,1} 	& \dots	 & a_{13,2000}
		\end{bmatrix}
		$
	\end{center}
	\vspace{0.02cm}
	
	\noindent Afterwards, to use minibatch processing it was necessary to reformat $X_{aco}$ into a vector $x_{aco}$ with the shape $1 \times 26000$.
	
%	\subsubsection*{Minibatch processing}
%	batchList = [ (1Batch-Features, 1Batch-MFCC, 1Batch-Labels), (2Batch-Features, 2Batch-MFCC, 2Batch-Labels), ...]
	
\section{Baseline Systems}
The architecture of the proposed \textit{AcoLex} system is depicted in Figure \ref{pic:sysArchitecture}. In this section we will explain the complete architecture of the system. Furthermore, we will discuss its two core components: the lexical and the acoustic model.

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{Bilder/acolex_CNN_paper.png}
	\caption{bla bla bla}
	\label{pic:sysArchitecture}
	%todo Bild verbessern, maxpooling bei LM hinzuf√ºgen
\end{figure}

\subsection{Lexical Model}
%todo cite Kim
The lexical model (LM), depicted on the right side in Figure \ref{pic:sysArchitecture}, consists of three layers. The first layer is an \textit{embedding layer} $E$ followed by a \textit{convolution layer} which uses three different filter sizes and finally a \textit{max-pooling layer}. The \textit{embedding layer} yields the same \textit{embedding matrix} as explained in Section \ref{sec:lexicalFeatures}. The filters of the \textit{convolution layer} capture three different word contexts, namely \textit{2-Word-}, \textit{3-Word-} and \textit{4-Word-Contexts}. Overall 300 filters are applied in the LM, were each filter type is used 100 times. After the convolution the outputs are passed to a \textit{max-pooling layer} which returns the highest value of each filter output. Therefore, the final output of the LM are three 100 element long vectors.

\subsection{Acoustic Model}
%todo cite Ortega
%The general structure of our \textit{convolutional neural network} consists of three stages. The first one is the \textit{input dependent} stage which can use the acoustic and/or lexical inputs and extracts useful features for the classification. Following that, the second stage concatenates the outputs into one vector which then is passed to the third where the utterance, with the help of a \textit{softmax layer}, is classified. The complete architecture is depicted in Figure \ref{pic:sysArchitecture}.

\subsection{AcoLex Model}

\section{Results}
%todo Tabelle mit der verwendeten Konfiguration

\section{Research Question: None}

\section{Conclusion}

\bibliographystyle{abbrv}
\bibliography{bibliography}
\nocite{*}
%todo Own cite style

\end{document}

