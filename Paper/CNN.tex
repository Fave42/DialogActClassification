%
% File emnlp2015.tex
%
% Contact: daniele.pighin@gmail.com
%%
%% Based on the style files for ACL-2015, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{lipsum}
\usepackage{natbib}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{amsmath}

%\setlength\titlebox{5cm}

\title{Report: Dialog Act Classification\\\textit{using Word Embeddings \& Acoustic Features}}

\author{Jens Beck \\
  {\tt jens.beckl@ims} \\\And
  Fabian Fey \\
  {\tt fabian.fey@ims} \\\And
  Richard Kollotzek \\
  {\tt richard.kollotzek@ims} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
\lipsum[2-2]
\end{abstract}

\section{Introduction}
The general task is to classify lexical and auditory speech into one of four predefined \textit{dialog act classes}. A \textit{dialog act}, in this context, represents informal information of how a dialog system should respond to a users input. The four provided classes are \textit{statement, opinion, question} and \textit{backchannel}. To solve this task we developed \textit{convolutional neural networks} (CNN) that use lexical and acoustic features. For the development and training of the systems a subset of the \textit{Switchboard Dialog Act Corpus} was used. In next chapters we discuss the development of the systems and subsequently to that the research question \textbf{INSERT HERE}.

\section{Data \& Data Preperation}
In this section we discuss the \textit{Switchboard Dialog Act Corpus} and the extraction of the lexical and acoustic features.

	\subsection{The Switchboard Dialog Act Corpus}
	The \textit{Switchboard Dialog Act Corpus} \cite{switchboard}, from now on abbreviated as \textit{SwDA}, consists of recordings with corresponding transcripts. Each of these recordings is assigned to one of 42 \textit{dialog act classes}. For this project we reduced the amount of classes down to four which are \textit{statement, opinion, question} and \textit{backchannel}. These classes are supersets of the 42 \textit{dialog act classes} defined in the \textit{SwDA}. The distribution of the four classes within the training, development and test set are shown in Table \ref{tab:dataDistribution}.
	%todo Examples for the four classes if more text is needed
	\begin{table}[]
		\begin{tabular}{ r | c c c }
			& training set & dev set & test set \\
			\hline
			opinion ($\sim$17\%) & 4984 & 1068 & 1070 \\
			question ($\sim$8\%) & 2150 & 460 & 463 \\
			backchannel ($\sim$24\%) & 6792 & 1455 & 1458  \\
			statement ($\sim$51\%) & 14459 & 3098 &  3099  \\
			\hline
			sum & 28385 & 6081 & 6090 \\
		\end{tabular}
		\caption{Displays the distribution of the four classes in the three data sets.}
		\label{tab:dataDistribution}
	\end{table}	
	The numbers illustrate a huge imbalance between the \textit{statement} class and the other three classes. However, we decided against reducing the data into equally distributed sets because this would exclude at least one third of the training data. This is important to keep in mind for the evaluation of the systems because an educated guess would have an accuracy of around 51\% by assigning all test examples to the \textit{statement} class.

	\subsection{Input Data Generation}
	Lexical and acoustic features were employed in our systems and had to be extracted and formatted into a machine readable format. For the lexical features we decided to use \textit{Google's} freely accessible word embeddings which were trained on 100 billion words \cite{word2vec}. As for the acoustic features we relied on \textit{Mel Frequency Cepstral Coefficient} (MFCC) features which were extracted with the \textit{openSMILE} feature extraction tool \cite{opensmile}. To ensure that data of different utterances could not be mixed the lexical and acoustic inputs were always stored with their respective one-hot vector in a triple data structure.
	
	\subsubsection*{Lexical Features}
	The word embedding matrix $E$ was generated by assigning each word to its corresponding 300 dimensional vector of the \textit{Google word2vec} model. If a word was not included in the model it was assigned a randomly generated 300 dimensional vector. Furthermore, we introduced a padding vector for the case that a sentence was shorter than our maximum sentence length. We decided to restrict the length of a single utterance to 100 words to not exclude to much lexical features for long utterances. The final size of the embedding matrix $E$ was $11825  \times 300$.
	\begin{center}
		$ E= 
		\begin{bmatrix}
		v_{1,1}	&  \dots	 & v_{1,300}    \\
		v_{2,1}	 	& \dots  & v_{2,300} 	\\
		\vdots	 	& \ddots & \vdots		\\
		v_{i,1} 	& \dots	 & v_{i,300}
		\end{bmatrix}
		$
	\end{center}
	\vspace{0.3cm}
	The lexical input for our systems is a vector $x_{lex}$ were each word is represented by the index $i$ of its corresponding vector in the embedding matrix $E$. The vector $x_{lex}$ has a length of 100. Each element represents the index of a word in the utterance. The vector $x_{lex}$ has the following shape: $x_{lex} = [i_{1}, i_{2}, ..., i_{100}]$ where $i$ represents the index.
	
	\subsubsection*{Acoustic Features}
	Each utterance had its acoustic features formatted into a matrix $X_{aco}$ of shape $13 \times 2000$. This matrix was generated by arranging 2000 \textit{MFCC-frames} into a matrix. Each \textit{frame} hereby consisted of 13 coefficients. The chosen \textit{frames} were the first and last thousand \textit{frames} of the audio recording. If a recording had less than 2000 \textit{MFCC-frames} the matrix was padded with zero vectors.
	
	\begin{center}
		$ X_{aco}= 
		\begin{bmatrix}
		a_{1,1}	&  \dots	 & a_{1,2000}   \\
		a_{2,1}	 	& \dots  & a_{2,2000} 	\\
		\vdots	 	& \ddots & \vdots		\\
		a_{13,1} 	& \dots	 & a_{13,2000}
		\end{bmatrix}
		$
	\end{center}
	\vspace{0.02cm}
	
	\noindent To use minibatch processing it was necessary to reformat $X_{aco}$ into a vector $x_{aco}$ with the shape $1 \times 26000$.
	
%	\subsubsection*{Minibatch processing}
%	batchList = [ (1Batch-Features, 1Batch-MFCC, 1Batch-Labels), (2Batch-Features, 2Batch-MFCC, 2Batch-Labels), ...]
	
\section{Baseline Systems}

\section{Results}

\section{Research Question: None}

\section{Conclusion}

\bibliographystyle{abbrv}
\bibliography{bibliography}
\nocite{*}
%todo Own cite style

\end{document}

