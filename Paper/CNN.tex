%
% File emnlp2015.tex
%
% Contact: daniele.pighin@gmail.com
%%
%% Based on the style files for ACL-2015, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{lipsum}
\usepackage{natbib}
\usepackage[font=small, labelfont=bf]{caption}

%\setlength\titlebox{5cm}

\title{Report: Dialog Act Classification\\\textit{using Word Embeddings \& Acoustic Features}}

\author{Jens Beck \\
  {\tt jens.beckl@ims} \\\And
  Fabian Fey \\
  {\tt fabian.fey@ims} \\\And
  Richard Kollotzek \\
  {\tt richard.kollotzek@ims} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
\lipsum[2-2]
\end{abstract}

\section{Introduction}
The general task is to classify lexical and auditory speech into one of four predefined \textit{dialog act classes}. A \textit{dialog act}, in this context, represents informal information of how a dialog system should respond to a users input. The four provided classes are \textit{statement, opinion, question} and \textit{backchannel}. To solve this task we developed \textit{convolutional neural networks} (CNN) that use lexical and acoustic features. For the development and training of the systems a subset of the \textit{Switchboard Dialog Act Corpus} was used. In next chapters we discuss the development of the systems and subsequently to that the research question \textbf{INSERT HERE}.

\section{Data \& Data Preperation}
In this section we discuss the \textit{Switchboard Dialog Act Corpus} and the extraction of the lexical and acoustic features.

	\subsection{The Switchboard Dialog Act Corpus}
	The \textit{Switchboard Dialog Act Corpus} \cite{switchboard}, from now on abbreviated as \textit{SwDA}, consists of recordings with corresponding transcripts. Each of these recordings is assigned to one of 42 \textit{dialog act classes}. For this project we reduced the amount of classes down to four which are \textit{statement, opinion, question} and \textit{backchannel}. These classes are supersets of the 42 \textit{dialog act classes} defined in the \textit{SwDA}. The distribution of the four classes within the training, development and test set are shown in Table \ref{tab:dataDistribution}.
	%todo Examples for the four classes if more text is needed
	\begin{table}[]
		\begin{tabular}{ r | c c c }
			& training set & dev set & test set \\
			\hline
			opinion ($\sim$17\%) & 4984 & 1068 & 1070 \\
			question ($\sim$8\%) & 2150 & 460 & 463 \\
			backchannel ($\sim$24\%) & 6792 & 1455 & 1458  \\
			statement ($\sim$51\%) & 14459 & 3098 &  3099  \\
			\hline
			sum & 28385 & 6081 & 6090 \\
		\end{tabular}
		\caption{Displays the distribution of the four classes in the three data sets.}
		\label{tab:dataDistribution}
	\end{table}	
	The numbers illustrate a huge imbalance between the \textit{statement} class and the other three classes. However, we decided against reducing the data into equally distributed sets because this would exclude at least one third of the training data. This is important to keep in mind for the evaluation of the systems because an educated guess would have an accuracy of around 51\% by assigning all test examples to the \textit{statement} class.

	\subsection{Input Data Generation}
	Lexical and acoustic features were employed in our systems and had to be extracted and formatted into a machine readable format. For the lexical features we decided to use \textit{Google's} freely accessible word embeddings which were trained on 100 billion words \cite{word2vec}. As for the acoustic features we relied on \textit{Mel Frequency Cepstral Coefficient} (MFCC) features which were extracted with the \textit{openSMILE} feature extraction tool \cite{opensmile}.
	
	The word embedding matrix was generated by assigning each word to its corresponding 300 dimensional vector of the \textit{Google word2vec} model. If a word was not included in the model it was assigned a randomly generated 300 dimensional vector.	Furthermore, we introduced an embedding vector representing the case that a sentence was shorter than our maximum sentence length. We decided to restrict the length of a single utterance to 100 words, to not exclude to much lexical features for long utterances.
	
	For the representation of each sentence every word was assigned a fixed index, this meant that every sentence was stored as a sequence of numbers.
	
	For the representation of the utterances we used a vector representation were each word was represented by the index of its vector in the embedding matrix. So each utterance was represented by a vector with 100 elements were each element was the index a word.
	

\section{Baseline Systems}

\section{Results}

\section{Research Question: None}

\section{Conclusion}

\bibliographystyle{abbrv}
\bibliography{bibliography}
\nocite{*}
%todo Own cite style

\end{document}

